{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the simplest Graph\n",
    "\n",
    "We start with a graph with two nodes connected by one edge. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.0.24)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.16 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langgraph) (0.1.23)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.16->langgraph) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.16->langgraph) (4.2.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.16->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<0.0.88,>=0.0.87 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.16->langgraph) (0.0.87)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.16->langgraph) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.16->langgraph) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.16->langgraph) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.16->langgraph) (8.2.3)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.16->langgraph) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.16->langgraph) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.16->langgraph) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.16->langgraph) (4.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/codespace/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.16->langgraph) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.16->langgraph) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.16->langgraph) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.16->langgraph) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.16->langgraph) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.16->langgraph) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes act like functions that can be called as needed. In our case Node 1 is our starting point and Node 2 is our finish point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    return input_1 + \" Hi \"\n",
    "\n",
    "def function_2(input_2):\n",
    "    return input_2 + \"there\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "# Define a Langchain graph\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"node_1\", function_1)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "\n",
    "workflow.add_edge('node_1', 'node_2')\n",
    "\n",
    "workflow.set_entry_point(\"node_1\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Hi there'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 'Hello Hi 'from node 'node_1':\n",
      "\n",
      "---\n",
      "\n",
      "Output 'Hello Hi there'from node 'node_2':\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = 'Hello'\n",
    "for output in app.stream(input):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output '{value}'from node '{key}':\")\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see, we can run the nodes as functions and return some values from them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding LLM Call\n",
    "\n",
    "Now, let's make the first node as an \"Agent\" that can call Open AI models. We can use langchain to make this call easy for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.1.7)\n",
      "Requirement already satisfied: langchain_openai in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.0.6)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (2.0.27)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.20 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (0.0.20)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.22 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (0.1.23)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (0.0.87)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain_openai) (1.12.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain_openai) (0.6.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/codespace/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.22->langchain) (4.2.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.22->langchain) (23.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/codespace/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2023.12.25)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A usual call to ChatOpenAI model in LangChain is done as below:\n",
    "\n",
    "First set your API keys for OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/python/3.10.13/lib/python3.10/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-rKcLAkdmwzWdMdOTdlXFKZuLEcY8drtZJ2DsXZZkTC9sQoE0 https://xiaoai.plus/v1\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['OPENAI_API_BASE'] = os.getenv(\"OPENAI_API_BASE\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_BASE= os.environ.get(\"OPENAI_API_BASE\")\n",
    "# Now you can access your environment variables using os.environ\n",
    "print(OPENAI_API_KEY,OPENAI_API_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_0165350fbb', 'finish_reason': 'stop', 'logprobs': None}, id='run-8cdba97d-6e05-4a4a-8483-1f627d0fc9fb-0', usage_metadata={'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Set the model as ChatOpenAI\n",
    "model = ChatOpenAI(\n",
    "            openai_api_base=OPENAI_API_BASE,\n",
    "            openai_api_key=OPENAI_API_KEY,\n",
    "            temperature=0)\n",
    "\n",
    "#Call the model with a user message\n",
    "model.invoke('Hey there')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you just want to see the AI response, you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke('Hey there').content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Keeping that in mind, let's change the function 1 above so that we can send the user question to the model. Then we will send this response to function 2, which will add a short string and return to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    response = model.invoke(input_1)\n",
    "    return response.content\n",
    "\n",
    "def function_2(input_2):\n",
    "    return \"Agent Says: \" + input_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Langchain graph\n",
    "workflow = Graph()\n",
    "\n",
    "#calling node 1 as agent\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "\n",
    "workflow.add_edge('agent', 'node_2')\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agent Says: Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"Hey there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 'Hello! How can I assist you today?'is from node 'agent':\n",
      "\n",
      "---\n",
      "\n",
      "Output 'Agent Says: Hello! How can I assist you today?'is from node 'node_2':\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = 'Hey there'\n",
    "for output in app.stream(input):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output '{value}'is from node '{key}':\")\n",
    "        print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First functional Agent App - City Temperature\n",
    "\n",
    "### Step 1: Parse the city mentioned\n",
    "Let's extract the city that a user mentions in a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
    "        Nothing more, just the city name mentioned. Following is the user query: \" + input_1\n",
    "    response = model.invoke(complete_query)\n",
    "    return response.content\n",
    "\n",
    "def function_2(input_2):\n",
    "    return \"Agent Says: \" + input_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Langchain graph\n",
    "workflow = Graph()\n",
    "\n",
    "#calling node 1 as agent\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "\n",
    "workflow.add_edge('agent', 'node_2')\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agent Says: Las Vegas'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"What's the temperature in Las Vegas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Adding a weather API call\n",
    "\n",
    "What if we want the function 2 to take the city name and give us the weather for that city.\n",
    "\n",
    "Well we know that Open Weather Map is [integrated](https://python.langchain.com/docs/integrations/tools/openweathermap) into LangChain\n",
    "\n",
    "We need to install pyown, create an API key on the website of Open Weather Map (which takes a few hours to activate) and then run the cells below to get weather of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyowm in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.3.0)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/codespace/.local/lib/python3.10/site-packages (from pyowm) (2.31.0)\n",
      "Requirement already satisfied: geojson<3,>=2.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyowm) (2.5.0)\n",
      "Requirement already satisfied: PySocks<2,>=1.7.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyowm) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.20.0->pyowm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.20.0->pyowm) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3,>=2.20.0->pyowm) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2.20.0->pyowm) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyowm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import OpenWeatherMapAPIWrapper\n",
    "load_dotenv()\n",
    "os.environ[\"OPENWEATHERMAP_API_KEY\"] = os.environ.get(\"OPENWEATHERMAP_API_KEY\")\n",
    "\n",
    "weather = OpenWeatherMapAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Las Vegas, the current weather is as follows:\n",
      "Detailed status: broken clouds\n",
      "Wind speed: 5.14 m/s, direction: 310°\n",
      "Humidity: 30%\n",
      "Temperature: \n",
      "  - Current: 14.7°C\n",
      "  - High: 15.69°C\n",
      "  - Low: 13.71°C\n",
      "  - Feels like: 13.01°C\n",
      "Rain: {}\n",
      "Heat index: None\n",
      "Cloud cover: 75%\n"
     ]
    }
   ],
   "source": [
    "weather_data = weather.run(\"Las Vegas\")\n",
    "print(weather_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's integrate this into function 2 and call the function two as a \"tool\" or \"weather_agent\" instead of \"node_2\" in our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
    "        Nothing more, just the city name mentioned. Following is the user query: \" + input_1\n",
    "    response = model.invoke(complete_query)\n",
    "    return response.content\n",
    "\n",
    "def function_2(input_2):\n",
    "    weather_data = weather.run(input_2)\n",
    "    return weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "workflow = Graph()\n",
    "\n",
    "#calling node 1 as agent\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "\n",
    "workflow.add_edge('agent', 'tool')\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"tool\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Las Vegas, the current weather is as follows:\\nDetailed status: broken clouds\\nWind speed: 5.14 m/s, direction: 310°\\nHumidity: 30%\\nTemperature: \\n  - Current: 14.7°C\\n  - High: 15.69°C\\n  - Low: 13.71°C\\n  - Feels like: 13.01°C\\nRain: {}\\nHeat index: None\\nCloud cover: 75%'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"What's the temperature in Las Vegas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 'Las Vegas'is from node 'agent':\n",
      "\n",
      "---\n",
      "\n",
      "Output 'In Las Vegas, the current weather is as follows:\n",
      "Detailed status: broken clouds\n",
      "Wind speed: 5.14 m/s, direction: 310°\n",
      "Humidity: 30%\n",
      "Temperature: \n",
      "  - Current: 14.7°C\n",
      "  - High: 15.69°C\n",
      "  - Low: 13.71°C\n",
      "  - Feels like: 13.01°C\n",
      "Rain: {}\n",
      "Heat index: None\n",
      "Cloud cover: 75%'is from node 'tool':\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = \"What's the temperature in Las Vegas\"\n",
    "for output in app.stream(input):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output '{value}'is from node '{key}':\")\n",
    "        print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Adding another LLM Call to filter results\n",
    "\n",
    "What if we only want the temperature? But current setup gives us the full weather report. \n",
    "\n",
    "Well we can make another LLM call to filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_3(input_3):\n",
    "    complete_query = \"Your task is to provide info concisely based on the user query. Following is the user query: \" + \"user input\"\n",
    "    response = model.invoke(complete_query)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the issue is the user input is not available from node 2.\n",
    "\n",
    "Can we pass user input all along from first node to the last?\n",
    "\n",
    "Yes, we can use a dictionary and pass it between nodes (we could also use just a list, but dict makes it a bit easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign AgentState as an empty dict\n",
    "AgentState = {}\n",
    "\n",
    "# messages key will be assigned as an empty array. We will append new messages as we pass along nodes. \n",
    "AgentState[\"messages\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': []}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgentState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to have this state filled as:\n",
    "{'messages': [HumanMessage, AIMessage, ...]]}\n",
    "\n",
    "Also now we need to modify our functions to pass info along the new AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(state):\n",
    "    messages = state['messages']\n",
    "    user_input = messages[-1]\n",
    "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
    "                    Nothing more, just the city name mentioned. Following is the user query: \" + user_input\n",
    "    response = model.invoke(complete_query)\n",
    "    state['messages'].append(response.content) # appending AIMessage response to the AgentState\n",
    "    return state\n",
    "\n",
    "def function_2(state):\n",
    "    messages = state['messages']\n",
    "    agent_response = messages[-1]\n",
    "    weather = OpenWeatherMapAPIWrapper()\n",
    "    weather_data = weather.run(agent_response)\n",
    "    state['messages'].append(weather_data)\n",
    "    return state\n",
    "\n",
    "def function_3(state):\n",
    "    messages = state['messages']\n",
    "    user_input = messages[0]\n",
    "    available_info = messages[-1]\n",
    "    agent2_query = \"Your task is to provide info concisely based on the user query and the available information from the internet. \\\n",
    "                        Following is the user query: \" + user_input + \" Available information: \" + available_info\n",
    "    response = model.invoke(agent2_query)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "workflow = Graph()\n",
    "\n",
    "\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "workflow.add_node(\"responder\", function_3)\n",
    "\n",
    "workflow.add_edge('agent', 'tool')\n",
    "workflow.add_edge('tool', 'responder')\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"responder\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature in Las Vegas is 14.7°C with broken clouds and a 75% cloud cover.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [\"what is the temperature in las vegas\"]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output '{'messages': ['what is the temperature in las vegas', 'Las Vegas']}'is from node 'agent':\n",
      "\n",
      "---\n",
      "\n",
      "Output '{'messages': ['what is the temperature in las vegas', 'Las Vegas', 'In Las Vegas, the current weather is as follows:\\nDetailed status: broken clouds\\nWind speed: 5.14 m/s, direction: 310°\\nHumidity: 30%\\nTemperature: \\n  - Current: 14.7°C\\n  - High: 15.69°C\\n  - Low: 13.71°C\\n  - Feels like: 13.01°C\\nRain: {}\\nHeat index: None\\nCloud cover: 75%']}'is from node 'tool':\n",
      "\n",
      "---\n",
      "\n",
      "Output 'The current temperature in Las Vegas is 14.7°C with broken clouds and a 75% cloud cover.'is from node 'responder':\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = {\"messages\": [\"what is the temperature in las vegas\"]}\n",
    "for output in app.stream(input):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output '{value}'is from node '{key}':\")\n",
    "        print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we notice that there is a lot of appending to the array going on, we can make it a bit easier with the following:\n",
    "\n",
    "```bash\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It basically makes the state dictionary as saw previously, and also makes sure that any new message is appended to the messages array when we do the following: \n",
    "```bash\n",
    "{\"messages\": [new_array_element]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### We also realize that our app is not capable of answering simple questions like \"how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am an AI assistant, so I don't have feelings, but thank you for asking. In Istanbul, the current weather is broken clouds with a temperature of 2.35°C, feeling like -2.77°C. The wind speed is 6.69 m/s with a direction of 40° and humidity is at 75%.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [\"how are you?\"]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because we always want to parse a city and then find the weather. \n",
    "\n",
    "We can make our agent smarter by saying only use the tool when needed, if not just respond back to the user. \n",
    "\n",
    "The way we can do this LangGraph is:\n",
    "1. binding a tool to the agent\n",
    "2. adding a conditional edge to the agent with the option to either call the tool or not\n",
    "3. defining the criteria for the conditional edge as when to call the tool. We will define a function for this.\n",
    "\n",
    "\n",
    "Let's start with the AgentState definition as mentioned a few cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binding tool with agent (LLM Model) is made easy in langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_community.tools.openweathermap import OpenWeatherMapQueryRun\n",
    "\n",
    "tools = [OpenWeatherMapQueryRun()]\n",
    "\n",
    "model = ChatOpenAI(\n",
    "            openai_api_base=OPENAI_API_BASE,\n",
    "            openai_api_key=OPENAI_API_KEY,\n",
    "            temperature=0)\n",
    "functions = [convert_to_openai_function(t) for t in tools]\n",
    "model = model.bind_tools(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our modified function_1 now becomes as below. The reason is, we are passing the human message as state and appending response to the state. Also, our agent now has a tool bound to it, that it can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(state):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For function 2, we want it to setup a tool and call it. It's made easy to invoke a tool in LangChain by using ToolInvocation and executing it with ToolNode . Then we respond back as a FunctionMessage so that our agent (node 1) knows that the tool was used and a response from tool is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\陈群\\AppData\\Local\\Temp\\ipykernel_13384\\391848375.py:6: LangGraphDeprecationWarning: ToolExecutor is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
      "  tool_executor = ToolExecutor(tools)\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import ToolInvocation, ToolExecutor  # 引入 ToolNode\n",
    "import json\n",
    "# from langchain_core.messages import FunctionMessage\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "def function_2(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]  # 取最后一条消息，获取要发送给工具的查询\n",
    "\n",
    "    # 确保 tool_calls 存在且为非空列表\n",
    "    tool_calls = last_message.additional_kwargs.get(\"tool_calls\", [])\n",
    "\n",
    "    if tool_calls:\n",
    "        function_data = tool_calls[0].get(\"function\", {})  # 获取 function 字典\n",
    "        arguments_str = function_data.get(\"arguments\", \"{}\")  # 获取 arguments JSON 字符串\n",
    "        parsed_tool_input = json.loads(arguments_str)  # 解析 JSON\n",
    "        tool_call_id=tool_calls[0][\"id\"]\n",
    "    else:\n",
    "        print(\"Warning: tool_calls is empty.\")\n",
    "    \n",
    "    # 构造 ToolInvocation\n",
    "    action = ToolInvocation(\n",
    "        tool=function_data[\"name\"],\n",
    "        tool_input=parsed_tool_input['location'],\n",
    "    )\n",
    "\n",
    "    # 使用 tool_executor 处理请求\n",
    "    response = tool_executor.invoke(action)\n",
    "    # 构造 FunctionMessage\n",
    "    function_message = ToolMessage(response, tool_call_id=tool_call_id)\n",
    "    # 返回消息列表\n",
    "    return {\"messages\": [function_message]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a function for the conditional edge, to help us figure out which direction to go (tool or user response)\n",
    "\n",
    "We can benefit from the agent (LLM) response in LangChain, which has additional_kwargs to make a function_call with the name of the tool.\n",
    "\n",
    "So our logic is, if function_call available in the additional_kwargs, then call tool if not then end the discussion and respond back to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def where_to_go(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    if \"tool_calls\" in last_message.additional_kwargs:\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with all of the changes above, our LangGraph app is modified as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langgraph.graph import Graph, END\n",
    "\n",
    "# workflow = Graph()\n",
    "\n",
    "# Or you could import StateGraph and pass AgentState to it\n",
    "from langgraph.graph import StateGraph, END\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "\n",
    "# The conditional edge requires the following info below.\n",
    "# First, we define the start node. We use `agent`.\n",
    "# This means these are the edges taken after the `agent` node is called.\n",
    "# Next, we pass in the function that will determine which node is called next, in our case where_to_go().\n",
    "\n",
    "workflow.add_conditional_edges(\"agent\", where_to_go,{   # Based on the return from where_to_go\n",
    "                                                        # If return is \"continue\" then we call the tool node.\n",
    "                                                        \"continue\": \"tool\",\n",
    "                                                        # Otherwise we finish. END is a special node marking that the graph should finish.\n",
    "                                                        \"end\": END\n",
    "                                                    }\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that if `tool` is called, then it has to call the 'agent' next. \n",
    "workflow.add_edge('tool', 'agent')\n",
    "\n",
    "# Basically, agent node has the option to call a tool node based on a condition, \n",
    "# whereas tool node must call the agent in all cases based on this setup.\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also pass the first message using HumanMessage component available in langchain, makes it easy to differentiate from AIMessage, and FunctionMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\陈群\\AppData\\Local\\Temp\\ipykernel_13384\\391848375.py:24: LangGraphDeprecationWarning: ToolInvocation is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
      "  action = ToolInvocation(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in las vegas?\")]}#what is the temperature in las vegas\n",
    "result = app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent':\n",
      "---\n",
      "{'messages': [AIMessage(content=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you with anything you need! How can I help you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 77, 'total_tokens': 113, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_0165350fbb', 'finish_reason': 'stop', 'logprobs': None}, id='run-aaa97084-8245-4ac9-83fa-bd145e74a9f7-0', usage_metadata={'input_tokens': 77, 'output_tokens': 36, 'total_tokens': 113, 'input_token_details': {}, 'output_token_details': {}})]}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"how are you today?\")]}#what is the temperature in las vegas\n",
    "for output in app.stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent':\n",
      "---\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'chatcmpl-Buc2CfgN8ehT7rvVwdlZKjJABHVR8', 'function': {'arguments': '{\"location\":\"Las Vegas\"}', 'name': 'open_weather_map'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 80, 'total_tokens': 96, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_0165350fbb', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-fc826765-2a9f-4f15-aecc-b3f7036f1197-0', tool_calls=[{'name': 'open_weather_map', 'args': {'location': 'Las Vegas'}, 'id': 'chatcmpl-Buc2CfgN8ehT7rvVwdlZKjJABHVR8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 80, 'output_tokens': 16, 'total_tokens': 96, 'input_token_details': {}, 'output_token_details': {}})]}\n",
      "\n",
      "---\n",
      "\n",
      "last_message===\n",
      " content='' additional_kwargs={'tool_calls': [{'id': 'chatcmpl-Buc2CfgN8ehT7rvVwdlZKjJABHVR8', 'function': {'arguments': '{\"location\":\"Las Vegas\"}', 'name': 'open_weather_map'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 80, 'total_tokens': 96, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_0165350fbb', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-fc826765-2a9f-4f15-aecc-b3f7036f1197-0' tool_calls=[{'name': 'open_weather_map', 'args': {'location': 'Las Vegas'}, 'id': 'chatcmpl-Buc2CfgN8ehT7rvVwdlZKjJABHVR8', 'type': 'tool_call'}] usage_metadata={'input_tokens': 80, 'output_tokens': 16, 'total_tokens': 96, 'input_token_details': {}, 'output_token_details': {}}\n",
      "parsed_tool_input===\n",
      " {'location': 'Las Vegas'}\n",
      "function_data===\n",
      " {'arguments': '{\"location\":\"Las Vegas\"}', 'name': 'open_weather_map'} open_weather_map\n",
      "parsed_tool_input===\n",
      " {'location': 'Las Vegas'} Las Vegas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\陈群\\AppData\\Local\\Temp\\ipykernel_13384\\1869951209.py:28: LangGraphDeprecationWarning: ToolInvocation is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
      "  action = ToolInvocation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response===\n",
      " In Las Vegas, the current weather is as follows:\n",
      "Detailed status: clear sky\n",
      "Wind speed: 5.36 m/s, direction: 0°\n",
      "Humidity: 35%\n",
      "Temperature: \n",
      "  - Current: 13.53°C\n",
      "  - High: 14.51°C\n",
      "  - Low: 10.88°C\n",
      "  - Feels like: 11.85°C\n",
      "Rain: {}\n",
      "Heat index: None\n",
      "Cloud cover: 0%\n",
      "function_message==\n",
      " content='In Las Vegas, the current weather is as follows:\\nDetailed status: clear sky\\nWind speed: 5.36 m/s, direction: 0°\\nHumidity: 35%\\nTemperature: \\n  - Current: 13.53°C\\n  - High: 14.51°C\\n  - Low: 10.88°C\\n  - Feels like: 11.85°C\\nRain: {}\\nHeat index: None\\nCloud cover: 0%' tool_call_id='chatcmpl-Buc2CfgN8ehT7rvVwdlZKjJABHVR8'\n",
      "Output from node 'tool':\n",
      "---\n",
      "{'messages': [ToolMessage(content='In Las Vegas, the current weather is as follows:\\nDetailed status: clear sky\\nWind speed: 5.36 m/s, direction: 0°\\nHumidity: 35%\\nTemperature: \\n  - Current: 13.53°C\\n  - High: 14.51°C\\n  - Low: 10.88°C\\n  - Feels like: 11.85°C\\nRain: {}\\nHeat index: None\\nCloud cover: 0%', tool_call_id='chatcmpl-Buc2CfgN8ehT7rvVwdlZKjJABHVR8')]}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'agent':\n",
      "---\n",
      "{'messages': [AIMessage(content='The current temperature in Las Vegas is 13.53°C.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 203, 'total_tokens': 217, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_0165350fbb', 'finish_reason': 'stop', 'logprobs': None}, id='run-8bcc8973-7c2c-452b-a5a3-f106f63629b1-0', usage_metadata={'input_tokens': 203, 'output_tokens': 14, 'total_tokens': 217, 'input_token_details': {}, 'output_token_details': {}})]}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in las vegas?\")]}#what is the temperature in las vegas\n",
    "for output in app.stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, that gives you a good understanding of how we built a LangGraph app and why we used different LC components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面给出完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\陈群\\AppData\\Local\\Temp\\ipykernel_13384\\3737097709.py:24: LangGraphDeprecationWarning: ToolExecutor is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
      "  tool_executor = ToolExecutor(tools)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_message===\n",
      " content='' additional_kwargs={'tool_calls': [{'id': 'chatcmpl-c8tdWcPD6h68XZXYEX3I1lIDLPZn6', 'function': {'arguments': '{\"location\":\"Las Vegas\"}', 'name': 'open_weather_map'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 80, 'total_tokens': 96, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_0165350fbb', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-53715c56-b036-4721-a09e-376a950fbf5c-0' tool_calls=[{'name': 'open_weather_map', 'args': {'location': 'Las Vegas'}, 'id': 'chatcmpl-c8tdWcPD6h68XZXYEX3I1lIDLPZn6', 'type': 'tool_call'}] usage_metadata={'input_tokens': 80, 'output_tokens': 16, 'total_tokens': 96, 'input_token_details': {}, 'output_token_details': {}}\n",
      "parsed_tool_input===\n",
      " {'location': 'Las Vegas'}\n",
      "function_data===\n",
      " {'arguments': '{\"location\":\"Las Vegas\"}', 'name': 'open_weather_map'} open_weather_map\n",
      "parsed_tool_input===\n",
      " {'location': 'Las Vegas'} Las Vegas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\陈群\\AppData\\Local\\Temp\\ipykernel_13384\\3737097709.py:46: LangGraphDeprecationWarning: ToolInvocation is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
      "  action = ToolInvocation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response===\n",
      " In Las Vegas, the current weather is as follows:\n",
      "Detailed status: clear sky\n",
      "Wind speed: 5.36 m/s, direction: 0°\n",
      "Humidity: 35%\n",
      "Temperature: \n",
      "  - Current: 13.53°C\n",
      "  - High: 14.51°C\n",
      "  - Low: 10.88°C\n",
      "  - Feels like: 11.85°C\n",
      "Rain: {}\n",
      "Heat index: None\n",
      "Cloud cover: 0% \n",
      " open_weather_map\n",
      "type result=====\n",
      "\n",
      "\n",
      " <class 'langgraph.pregel.io.AddableValuesDict'>\n",
      "result=====\n",
      "\n",
      "\n",
      " {'messages': [HumanMessage(content='what is the temperature in las vegas?', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'chatcmpl-c8tdWcPD6h68XZXYEX3I1lIDLPZn6', 'function': {'arguments': '{\"location\":\"Las Vegas\"}', 'name': 'open_weather_map'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 80, 'total_tokens': 96, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_0165350fbb', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-53715c56-b036-4721-a09e-376a950fbf5c-0', tool_calls=[{'name': 'open_weather_map', 'args': {'location': 'Las Vegas'}, 'id': 'chatcmpl-c8tdWcPD6h68XZXYEX3I1lIDLPZn6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 80, 'output_tokens': 16, 'total_tokens': 96, 'input_token_details': {}, 'output_token_details': {}}), ToolMessage(content='In Las Vegas, the current weather is as follows:\\nDetailed status: clear sky\\nWind speed: 5.36 m/s, direction: 0°\\nHumidity: 35%\\nTemperature: \\n  - Current: 13.53°C\\n  - High: 14.51°C\\n  - Low: 10.88°C\\n  - Feels like: 11.85°C\\nRain: {}\\nHeat index: None\\nCloud cover: 0%', tool_call_id='chatcmpl-c8tdWcPD6h68XZXYEX3I1lIDLPZn6'), AIMessage(content='The current temperature in Las Vegas is 13.53°C.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 203, 'total_tokens': 217, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_0165350fbb', 'finish_reason': 'stop', 'logprobs': None}, id='run-d115a20e-e9fd-4ac4-bda3-2c2fbc433dad-0', usage_metadata={'input_tokens': 203, 'output_tokens': 14, 'total_tokens': 217, 'input_token_details': {}, 'output_token_details': {}})]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langgraph.graph import Graph, END,StateGraph\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_community.tools.openweathermap import OpenWeatherMapQueryRun\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langgraph.prebuilt import ToolInvocation, ToolNode,ToolExecutor  # 引入 ToolNode\n",
    "\n",
    "tools = [OpenWeatherMapQueryRun()]\n",
    "\n",
    "model = ChatOpenAI(\n",
    "            openai_api_base=OPENAI_API_BASE,\n",
    "            openai_api_key=OPENAI_API_KEY,\n",
    "            temperature=0)\n",
    "\n",
    "functions = [convert_to_openai_function(t) for t in tools]\n",
    "\n",
    "model = model.bind_tools(functions)\n",
    "\n",
    "def function_1(state):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "def function_2(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]  # 取最后一条消息，获取要发送给工具的查询\n",
    "    print('last_message===\\n',last_message)\n",
    "\n",
    "    # 确保 tool_calls 存在且为非空列表\n",
    "    tool_calls = last_message.additional_kwargs.get(\"tool_calls\", [])\n",
    "\n",
    "    if tool_calls:\n",
    "        function_data = tool_calls[0].get(\"function\", {})  # 获取 function 字典\n",
    "        arguments_str = function_data.get(\"arguments\", \"{}\")  # 获取 arguments JSON 字符串\n",
    "        parsed_tool_input = json.loads(arguments_str)  # 解析 JSON\n",
    "        print('parsed_tool_input===\\n', parsed_tool_input)\n",
    "        tool_call_id=tool_calls[0][\"id\"]\n",
    "    else:\n",
    "        print(\"Warning: tool_calls is empty.\")\n",
    "    \n",
    "    print('function_data===\\n',function_data,function_data[\"name\"])\n",
    "    print('parsed_tool_input===\\n',parsed_tool_input,parsed_tool_input['location'])\n",
    "    # 构造 ToolInvocation\n",
    "    action = ToolInvocation(\n",
    "        tool=function_data[\"name\"],\n",
    "        tool_input=parsed_tool_input['location'],\n",
    "    )\n",
    "\n",
    "    # 使用 tool_executor 处理请求\n",
    "    response = tool_executor.invoke(action)\n",
    "    print('response===\\n',response,'\\n',action.tool)\n",
    "    # 构造 FunctionMessage\n",
    "    function_message = ToolMessage(response, tool_call_id=tool_call_id)\n",
    "\n",
    "    # 返回消息列表\n",
    "    return {\"messages\": [function_message]}\n",
    "\n",
    "def where_to_go(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    if \"tool_calls\" in last_message.additional_kwargs:\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "    \n",
    "\n",
    "# from langgraph.graph import Graph, END\n",
    "# workflow = Graph()\n",
    "\n",
    "# Or you could import StateGraph and pass AgentState to it\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "\n",
    "# The conditional edge requires the following info below.\n",
    "# First, we define the start node. We use `agent`.\n",
    "# This means these are the edges taken after the `agent` node is called.\n",
    "# Next, we pass in the function that will determine which node is called next, in our case where_to_go().\n",
    "\n",
    "workflow.add_conditional_edges(\"agent\", where_to_go,{   # Based on the return from where_to_go\n",
    "                                                        # If return is \"continue\" then we call the tool node.\n",
    "                                                        \"continue\": \"tool\",\n",
    "                                                        # Otherwise we finish. END is a special node marking that the graph should finish.\n",
    "                                                        \"end\": END\n",
    "                                                    }\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that if `tool` is called, then it has to call the 'agent' next. \n",
    "workflow.add_edge('tool', 'agent')\n",
    "\n",
    "# Basically, agent node has the option to call a tool node based on a condition, \n",
    "# whereas tool node must call the agent in all cases based on this setup.\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in las vegas?\")]} # what is the temperature in las vegas\n",
    "result = app.invoke(inputs)\n",
    "print('type result=====\\n\\n\\n',type(result))\n",
    "print('result=====\\n\\n\\n',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in Las Vegas is 13.53°C.\n"
     ]
    }
   ],
   "source": [
    "# 取出 messages 列表中第一个消息对象\n",
    "first_msg = result[\"messages\"][-1]\n",
    "\n",
    "# 获取第一个消息的 content 属性\n",
    "first_content = first_msg.content\n",
    "\n",
    "# 打印第一个消息的内容\n",
    "print(first_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
