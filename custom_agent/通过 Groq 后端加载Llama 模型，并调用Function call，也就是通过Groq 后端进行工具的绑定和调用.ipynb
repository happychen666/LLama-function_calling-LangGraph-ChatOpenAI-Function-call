{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pydantic import create_model\n",
    "import inspect, json\n",
    "from inspect import Parameter\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['OPENAI_API_BASE'] = os.getenv(\"OPENAI_API_BASE\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_BASE= os.environ.get(\"OPENAI_API_BASE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Custom agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abc(num1:int, num2:int)->int:\n",
    "    \"Compute abc between two numbers\"\n",
    "    return 2*(num1) - 2*(num2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonschema(f):\n",
    "    \"\"\"\n",
    "    Generate a JSON schema for the input parameters of the given function.\n",
    "\n",
    "    Parameters:\n",
    "        f (FunctionType): The function for which to generate the JSON schema.\n",
    "\n",
    "    Returns:\n",
    "        Dict: A dictionary containing the function name, description, and parameters schema.\n",
    "    \"\"\"\n",
    "    kw = {n: (o.annotation, ... if o.default == Parameter.empty else o.default)\n",
    "            for n, o in inspect.signature(f).parameters.items()}\n",
    "    s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n",
    "    return dict(name=f.__name__, description=f.__doc__, parameters=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(num1: int, num2: int) -> int\n",
      "OrderedDict([('num1', <Parameter \"num1: int\">), ('num2', <Parameter \"num2: int\">)])\n"
     ]
    }
   ],
   "source": [
    "def jsonschema1(f):\n",
    "    \"\"\"\n",
    "    Generate a JSON schema for the input parameters of the given function.\n",
    "\n",
    "    Parameters:\n",
    "        f (FunctionType): The function for which to generate the JSON schema.\n",
    "\n",
    "    Returns:\n",
    "        Dict: A dictionary containing the function name, description, and parameters schema.\n",
    "    \"\"\"\n",
    "    print(inspect.signature(f))\n",
    "    print(inspect.signature(f).parameters)\n",
    "    \n",
    "jsonschema1(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(a: int, b: int = 10, *args, **kwargs) -> bool:\n",
    "    pass\n",
    "\n",
    "jsonschema1(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_json = jsonschema(abc)\n",
    "abc_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_base=OPENAI_API_BASE,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    # tream=True,\n",
    "    temperature=0)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
    "]\n",
    "\n",
    "# Call the model with the messages\n",
    "response = chat.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(response)\n",
    "print('\\n')\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ask GPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_base=OPENAI_API_BASE,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    # tream=True,\n",
    "    temperature=0)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"use abc function to calculate value between 2 and 3\"},\n",
    "]\n",
    "\n",
    "# Call the model with the messages\n",
    "response = chat.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(response)\n",
    "print('\\n')\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model= model_name,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"compute abc between 2 and 3\"},\n",
    "  ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_base=OPENAI_API_BASE,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    temperature=0,\n",
    "    model_kwargs={\n",
    "        \"functions\": [abc_json],\n",
    "        \"function_call\": \"auto\"\n",
    "    }\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"use abc function to calculate value between 2 and 3\"},\n",
    "]\n",
    "\n",
    "# Call the model with the messages\n",
    "response = chat.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(response)\n",
    "print('\\n')\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(response.additional_kwargs)\n",
    "print(response.additional_kwargs['function_call'])\n",
    "print(response.additional_kwargs['function_call']['arguments'])\n",
    "print(response.additional_kwargs['function_call']['name'])\n",
    "func_name = response.additional_kwargs['function_call']['name']\n",
    "\n",
    "func_args_str = response.additional_kwargs['function_call']['arguments']\n",
    "func_args = json.loads(func_args_str)  # 转换为字典\n",
    "if func_name == 'abc':\n",
    "    result = abc(**func_args)\n",
    "print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Executing the function by extracting the info from the output of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.function_call)\n",
    "print(response.choices[0].message.function_call.arguments)\n",
    "print(type(response.choices[0].message.function_call.arguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_name = response.choices[0].message.function_call.name\n",
    "func_args = json.loads(response.choices[0].message.function_call.arguments)\n",
    "print(\"Function name:\", func_name)\n",
    "print(\"Function arguments:\", func_args)\n",
    "print(type(func_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if func_name == 'abc':\n",
    "    result = abc(**func_args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using Langchain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def abc(num1:int, num2:int)->int:\n",
    "    \"Compute abc between two numbers\"\n",
    "    return 2*(num1) - 2*(num2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pydantic import create_model\n",
    "import inspect, json\n",
    "from inspect import Parameter\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['OPENAI_API_BASE'] = os.getenv(\"OPENAI_API_BASE\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_BASE= os.environ.get(\"OPENAI_API_BASE\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def abc(num1:int, num2:int)->int:\n",
    "    \"Compute abc between two numbers\"\n",
    "    return 2*(num1) - 2*(num2)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",  # 使用支持函数调用的模型\n",
    "    openai_api_base=OPENAI_API_BASE,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "tools = [abc]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "messages = \"use abc function to calculate value between 2 and 3\"\n",
    "# [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"use abc function to calculate value between 2 and 3\"},\n",
    "# ]\n",
    "\n",
    "# Call the model with the messages\n",
    "response = llm_with_tools.invoke(messages)\n",
    "# response = llm.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(response)\n",
    "print('\\n')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [abc]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_with_tools.invoke(\"Compute abc between 2 and 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.additional_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# 如果没有设置 GROQ_API_KEY，提示用户输入 API 密钥\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n",
    "\n",
    "# 从 langchain.chat_models 导入 init_chat_model 来初始化 Llama 模型\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# 使用 @tool 装饰器注册工具函数\n",
    "@tool\n",
    "def abc(num1: int, num2: int) -> int:\n",
    "    \"Compute abc between two numbers\"\n",
    "    return 2 * num1 - 2 * num2\n",
    "\n",
    "# 初始化 Llama 模型，注意 model_provider 设置为 \"groq\"，并传入温度参数\n",
    "llm = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\", temperature=0)\n",
    "\n",
    "# 将工具绑定到 Llama 模型上\n",
    "tools = [abc]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# 构造结构化消息，包含 system 和 user 消息，帮助模型理解上下文\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"use abc function to calculate value between 2 and 3\"}\n",
    "]\n",
    "\n",
    "# 调用模型，模型会根据消息判断是否调用工具\n",
    "response = llm_with_tools.invoke(messages)\n",
    "\n",
    "# 输出完整响应、工具调用列表以及生成的文本内容\n",
    "print(\"Full Response:\", response)\n",
    "print(\"\\nTool Calls:\", response.tool_calls)\n",
    "print(\"\\nContent:\", response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response: The function `abcefikfs` was called with arguments `a=2` and `b=3`. The result of the function is `ffffffffff`.\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, ToolMessage,SystemMessage\n",
    "\n",
    "\n",
    "# 如果没有设置 GROQ_API_KEY，则提示用户输入\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n",
    "\n",
    "# 使用 @tool 装饰器注册工具函数\n",
    "@tool\n",
    "def abcefikfs(a: int, b: int) -> str:\n",
    "    \"\"\"abcefikfs do nothing.\"\"\"\n",
    "    return 'ffffff'\n",
    "\n",
    "# 初始化 Llama 模型，使用 Groq 后端\n",
    "llm = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\", temperature=0)\n",
    "\n",
    "# 将工具绑定到模型上\n",
    "tools = [abcefikfs]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "messages=[SystemMessage(content=\"请只返回最终结果，不要解释，如果没有找到结果，请返回无法找到答案\")]\n",
    "query = \"Use function abcefikfs with a=2 and b=3 and return the result.\"\n",
    "# print(llm_with_tools.invoke(query).tool_calls)\n",
    "\n",
    "messages = [HumanMessage(query)]\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"abcefikfs\": abcefikfs}[tool_call[\"name\"].lower()]\n",
    "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
    "    messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "    \n",
    "messages.append(HumanMessage(content=f\"The function returned: {tool_output}. Please summarize.\"))   \n",
    "# 第二次调用：将工具输出和额外提示传回给模型，生成最终答案\n",
    "response = llm.invoke(messages)\n",
    "print(\"\\nFinal Response:\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'无法找到答案'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不使用工具，将问题传给模型不用工具处理\n",
    "messages = [\n",
    "    SystemMessage(content=\"请只返回最终结果，不要解释，如果没有找到结果，请返回无法找到答案\"),\n",
    "    HumanMessage(content=\"Use function abcefikfs with a=2 and b=3 and return the result.\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env-name)",
   "language": "python",
   "name": "env-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
